{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_Improving_Performance_by_Wavelets_Denoising.ipynb","version":"0.3.2","provenance":[{"file_id":"16hGhkA8ZfS6jNFE3RNG4EYcDS4BRASmH","timestamp":1541519067088},{"file_id":"1sJA328dcutKrVTe4WvFARVMFZ8zX6_Wl","timestamp":1541514071626},{"file_id":"10T0p4FuaF4Bgph3dFRSYxbRMYkVPS9gN","timestamp":1541426296960},{"file_id":"1AzPfrMqrhUxgE8EN0WTPoJ39B5xIEx-a","timestamp":1541346109193},{"file_id":"1NEUvY8uKCaoht45hPQV_qOps7ZROw1l6","timestamp":1541264425550}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"KXCMLqVzKQ7F","colab_type":"text"},"cell_type":"markdown","source":["# Improving Performance by Wavelets Denoising\n","\n","This is continual series of assigment \n","\n","You can extend your own code from asssigment M3.2\n","\n","- Exercise. Wavelet Denoising for Return Prediction Model"]},{"metadata":{"id":"gP7BoQO-Z6Wk","colab_type":"text"},"cell_type":"markdown","source":["# Exercise. Wavelets Denoising for Return Prediction Model\n"]},{"metadata":{"id":"f549wKxKn6FK","colab_type":"text"},"cell_type":"markdown","source":["#### Requirements\n","- Apply Wavelets Denoising on the Return of Cryptocurrency\n","- Predict Denoised Future Return of Crypotocurrency \n","- Evaluate the Performance\n","\n","  note. be careful not to use future inforemation for wavelets\n","\n","#### Procedures\n","- Preprocessing\n","  1. Data Import and Create Balanced Panel\n","  2. Denoising Return and Create Target Variable\n","  3. Train / Test Split\n","  4. Create Sequences\n","  \n","- Training / Predicting Model\n","  1. Model Build\n","  2. Model Train\n","  3. Prediction\n","  4. Evaluation"]},{"metadata":{"id":"xIxPbmzGQFwR","colab_type":"text"},"cell_type":"markdown","source":["### 1. Data Import and Create Balanced Panel"]},{"metadata":{"id":"Cha8L6EJrm3p","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install pywavelets"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4T3G0OFBKcsJ","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TJVHhAmOMWmH","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"796JRrylMW2y","colab_type":"code","colab":{}},"cell_type":"code","source":["DATA_PATH = \"/content/gdrive/My Drive/Lecture/StudyPie/Data/\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"uV7aneomMlG6","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls \"/content/gdrive/My Drive/Lecture/StudyPie/Data/\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"cmkvrXr9MmQ6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Unzip Data\n","# It will take more than 5 min\n","import zipfile\n","import io\n","\n","zf = zipfile.ZipFile(DATA_PATH+\"crypto_data.zip\", \"r\")\n","zf.extractall(DATA_PATH)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7KTUsoWxNXqi","colab_type":"code","colab":{}},"cell_type":"code","source":["SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n","FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n","RATIO_TO_PREDICT = \"LTC-USD\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"m7vQBLJZq7j7","colab_type":"text"},"cell_type":"markdown","source":["## Preprocessing"]},{"metadata":{"id":"754AjNC3q97y","colab_type":"text"},"cell_type":"markdown","source":["### 1. Data Import and Create Balanced Panel"]},{"metadata":{"id":"ooR6vTP7q1LP","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","from sklearn import preprocessing \n","\n","main_df = pd.DataFrame() # begin empty\n","\n","ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n","\n","for ratio in ratios:  # begin iteration\n","  print(ratio)\n","  dataset = DATA_PATH+f'crypto_data/{ratio}.csv'  # get the full path to the file.\n","  df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n","\n","  # rename volume and close to include the ticker so we can still which close/volume is which:\n","  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n","\n","  df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n","  df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n","\n","  if len(main_df)==0:  # if the dataframe is empty\n","      main_df = df  # then it's just the current df\n","  else:  # otherwise, join this data to the main one\n","      main_df = main_df.join(df)\n","\n","main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n","main_df.dropna(inplace=True)\n","print(main_df.head())  # how did we do??"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aT2FA6Qzq4ev","colab_type":"text"},"cell_type":"markdown","source":["### 2. Create Target Variable"]},{"metadata":{"id":"zlKti7hGorl4","colab_type":"code","colab":{}},"cell_type":"code","source":["import pywt\n","import numpy as np\n","from statsmodels.robust import mad\n"," \n","def waveletSmooth(x, wavelet=\"haar\", level=1):\n","    # calculate the wavelet coefficients\n","    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n","    \n","    # calculate a threshold\n","    sigma = mad(coeff[-level])\n","    \n","    # changing this threshold also changes the behavior,\n","    # but I have not played with this very much\n","    uthresh = sigma * np.sqrt( 2*np.log(len(x)))\n","    coeff[1:] = (pywt.threshold( i, value=uthresh, mode=\"soft\") for i in coeff[1:])\n","    \n","    # reconstruct the signal using the thresholded coefficients\n","    y = pywt.waverec(coeff, wavelet, mode=\"per\" )\n","    return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QzAsHZIxq3rv","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","currency_targets = [\"BTC\"]\n","\n","for currency_target in currency_targets:\n","  main_df[currency_target+'-USD-TARGET'] = main_df[currency_target+'-USD_close'].shift(-FUTURE_PERIOD_PREDICT )\n","\n","  # scale up, you can do any other scaling methods!\n","  # how can we improve this part?\n","  # hint: reduce noise of future return  \n","  main_df[currency_target+'-USD-TARGET-RETURN'] = (main_df[currency_target+'-USD-TARGET']-main_df[currency_target+'-USD_close'])/main_df[currency_target+'-USD_close']*100 # scale up\n","  main_df = main_df[np.isfinite(main_df[currency_target+'-USD-TARGET-RETURN'])]\n","    \n","  \"\"\"\n","  \n","  ##### Write Code Here #####\n","  \n","  main_df[currency_target+'-USD-TARGET-RETURN'] => denoised value!\n","  \n","  note. be careful not to use future inforemation for wavelets\n","  \n","  \"\"\"\n","  main_df.drop(columns=[currency_target+'-USD-TARGET'], inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NSwmH4sMqNwj","colab_type":"code","colab":{}},"cell_type":"code","source":["#main_df[currency_target+'-USD-TARGET-RETURN'].plot()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7gA78XCvzUJ9","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import preprocessing  # pip install sklearn ... if you don't have it!\n","main_df.fillna(main_df.mean(), inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UtIx1zn1rEQ9","colab_type":"code","colab":{}},"cell_type":"code","source":["main_df.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"73dbkD2wrUCM","colab_type":"text"},"cell_type":"markdown","source":["### 3. Train / Test Split"]},{"metadata":{"id":"hdBJZalyrQBs","colab_type":"code","colab":{}},"cell_type":"code","source":["times = sorted(main_df.index.values)  # get the times\n","last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n","\n","test_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n","main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hhupZQWGrXYs","colab_type":"text"},"cell_type":"markdown","source":["### 4. Create Sequences"]},{"metadata":{"id":"1nZZOqKkrZ-N","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import preprocessing  # pip install sklearn ... if you don't have it!\n","from collections import deque\n","import random\n","import numpy as np\n","\n","def sequence_generator(main_df, SEQ_LEN, suffle=True,seed=101):\n","    \n","  sequential_data = []  # this is a list that will CONTAIN the sequences\n","  queue = deque(maxlen = SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n","\n","  for i in main_df.values:  # iterate over the values\n","      queue.append([n for n in i[:-1]])  # store all but the target\n","      if len(queue) == SEQ_LEN:  # make sure we have 60 sequences!\n","          sequential_data.append([np.array(queue), i[-1]])  # append those bad boys!\n","\n","  if suffle == True:\n","      random.seed(seed)\n","      random.shuffle(sequential_data)  # shuffle for good measure.\n","\n","  X = []\n","  y = []\n","\n","  for seq, target in sequential_data:  # going over our new sequential data\n","      X.append(seq)  # X is the sequences\n","      y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n","\n","  return np.array(X), y  # return X and y...and make X a numpy array!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WMPS0r6grcFs","colab_type":"code","colab":{}},"cell_type":"code","source":["train_x, train_y = sequence_generator(main_df , SEQ_LEN, suffle=True, seed=101)\n","test_x, test_y = sequence_generator(test_main_df , SEQ_LEN, suffle=True, seed=101)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NBTVnFZhrd5z","colab_type":"code","colab":{}},"cell_type":"code","source":["print(train_x.shape, len(train_y))\n","print(test_x.shape, len(test_y))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c4Wp-Efxrfoz","colab_type":"text"},"cell_type":"markdown","source":["## Return Prediction Model"]},{"metadata":{"id":"ZppjdkQmrjcD","colab_type":"text"},"cell_type":"markdown","source":["### 1. Model Build"]},{"metadata":{"id":"7UNi3N9SryVf","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LSTM,\\\n","CuDNNLSTM, BatchNormalization, Flatten, Activation\n","\n","def ex2_models(input_dim, output_dim):\n","\n","  # you can try your own model!\n","  \n","  L1 = 256  # 256\n","  L2 = 256  # 256\n","  L3 = 32  # 32\n","\n","  model = Sequential()\n","  \n","  model.add(CuDNNLSTM(L1, input_shape=input_dim, return_sequences=True))\n","  model.add(Dropout(0.2))\n","  model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n","\n","  model.add(CuDNNLSTM(L2, return_sequences=True))\n","  model.add(Dropout(0.1))\n","  model.add(BatchNormalization())\n","\n","  model.add(CuDNNLSTM(L3))\n","  model.add(Dropout(0.2))\n","  model.add(BatchNormalization())\n","\n","\n","  model.add(Flatten())\n","  model.add(Dense(output_dim))\n","  model.add(Activation('linear'))\n","\n","  model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n","            loss='mean_squared_error')\n","\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dw-1hJa0rppx","colab_type":"text"},"cell_type":"markdown","source":["### 2. Model Train"]},{"metadata":{"id":"skDP-L25rx8-","colab_type":"code","colab":{}},"cell_type":"code","source":["model2 = ex2_models(train_x.shape[1:], 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yP0GWYGar_RW","colab_type":"code","colab":{}},"cell_type":"code","source":["BATCH_SIZE = 64 \n","NUM_ITERATIONS = 10\n","\n","hist2 = model2.fit(train_x, train_y, \n","              validation_split=0.2,                   \n","              batch_size = BATCH_SIZE,\n","              epochs = NUM_ITERATIONS)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uUWrOqQfrvlY","colab_type":"text"},"cell_type":"markdown","source":["### 3. Prediction"]},{"metadata":{"id":"rrp4PaUvrxPZ","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions = model2.predict(test_x)\n","\n","# Score model\n","score = model2.evaluate(test_x, test_y,\n","                       verbose=0)\n","\n","print('Test loss:', score) # this is mean_squared_error "],"execution_count":0,"outputs":[]},{"metadata":{"id":"W4wWyk7nzFwL","colab_type":"text"},"cell_type":"markdown","source":["### 4. Evaluation"]},{"metadata":{"id":"RAVN3l32bUNa","colab_type":"text"},"cell_type":"markdown","source":["1. Comparing Accuracy both Train and Validation set data\n","  - Compare loss according to the below materials\n","  - Visualize training history  \n","  - Check genelarization of your model\n","  - Refer\n","    - https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-1-2-correct-time-series-forecasting-backtesting-9776bfd9e589\n","  - Visualization Hint\n","    - https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/"]},{"metadata":{"id":"rYlRBPNl4TM6","colab_type":"code","colab":{}},"cell_type":"code","source":["# list all data in history\n","print(hist2.history.keys())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OV0WQnOdWj9Y","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","# summarize history for accuracy\n","plt.plot(hist2.history['loss'])\n","plt.plot(hist2.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hjtq_F2bcDp6","colab_type":"text"},"cell_type":"markdown","source":["    \n","2. Measure Model Accuracy for Continuous Value\n","  - Change Your Prediction Values into Up / Down Binary Variable\n","  - After then do the same things as in Exercise 1\n","    - Accuracy, Recall, F1 Score based on Confusion Matrix\n","    - Refer definition of each scores\n","      - Confusion matrix https://en.wikipedia.org/wiki/Confusion_matrix\n","      - Confusion matrix in Korean https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/\n","  - Hint\n","    - http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n","    - http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html      "]},{"metadata":{"id":"8AgQBv8A_10R","colab_type":"code","colab":{}},"cell_type":"code","source":["predictions_binary = [1 if prediction >0 else 0 for prediction in predictions]\n","test_y_binary = [1 if y >0 else 0 for y in test_y]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c6yXDrBIcFPa","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import classification_report\n","print(classification_report(predictions_binary, test_y_binary))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mbgv-A7R6a8_","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","print(confusion_matrix(predictions_binary, test_y_binary))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WXoHVredcFg9","colab_type":"text"},"cell_type":"markdown","source":["3. Measure Test Set Return based on the Simplest Strategy\n","  - Condition\n","    - Initial budget = 100\n","  - Strategy\n","    - If we predict up, then buy or hold (if we already bought)\n","    - If we predict down, then sell (if we already bought) or do nothing \n","  - Draw your return\n"]},{"metadata":{"id":"KKOeqD44cG1b","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","for currency_target in currency_targets:\n","    bechmark_return = 100\n","    bechmark_return_history = []\n","\n","    prediction_return = 100\n","    prediction_return_history = []\n","\n","    buy_price = 0\n","    sell_price = 0\n","    \n","    hold = False\n","    \n","    test_main_df[currency_target+'-USD-FUTURE_close'] = test_main_df[currency_target+'-USD_close'].shift(-1)\n","    test_main_df[currency_target+'-USD-RETURN'] = (test_main_df[currency_target+'-USD-FUTURE_close'] \n","                                                                - test_main_df[currency_target+'-USD_close'])/test_main_df[currency_target+'-USD_close']\n","    \n","    for prediction, (i, r) in zip(predictions, test_main_df.iloc[SEQ_LEN-1:].iterrows()):\n","        \n","        if hold == True:\n","            prediction_return = prediction_return*(1+r[currency_target+'-USD-RETURN'])  \n","            \n","        bechmark_return = bechmark_return*(1+r[currency_target+'-USD-RETURN'])\n","        bechmark_return_history.append(bechmark_return)\n","        prediction_return_history.append(prediction_return)\n","\n","        if prediction >0:\n","            hold = True\n","        else: \n","            hold = False\n","    \n","    plt.title(currency_target)\n","    plt.plot(prediction_return_history, label = 'prediction return')    \n","    plt.plot(bechmark_return_history, label = 'becnmark return')\n","    plt.xticks(rotation=30)\n","    plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xwVMAettsh7H","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}